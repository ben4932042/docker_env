{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # for 畫圖用\n",
    "import pandas as pd\n",
    "\n",
    "# Import the training set\n",
    "dataset_train = pd.read_csv('/home/jovyan/Project/all_stock_price/public/2330.TW.csv')  # 讀取訓練集\n",
    "training_set = dataset_train.iloc[:, 1:2].values  # 取「Open」欄位值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>120.92</td>\n",
       "      <td>121.77</td>\n",
       "      <td>117.95</td>\n",
       "      <td>118.37</td>\n",
       "      <td>39169000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>117.95</td>\n",
       "      <td>118.80</td>\n",
       "      <td>116.25</td>\n",
       "      <td>117.10</td>\n",
       "      <td>46381000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>117.10</td>\n",
       "      <td>117.10</td>\n",
       "      <td>114.55</td>\n",
       "      <td>114.98</td>\n",
       "      <td>53617000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>114.13</td>\n",
       "      <td>114.55</td>\n",
       "      <td>110.73</td>\n",
       "      <td>112.86</td>\n",
       "      <td>62539000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>112.01</td>\n",
       "      <td>114.55</td>\n",
       "      <td>112.01</td>\n",
       "      <td>113.70</td>\n",
       "      <td>50123000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>112.86</td>\n",
       "      <td>114.13</td>\n",
       "      <td>110.73</td>\n",
       "      <td>112.86</td>\n",
       "      <td>48345000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>112.86</td>\n",
       "      <td>113.70</td>\n",
       "      <td>111.58</td>\n",
       "      <td>112.86</td>\n",
       "      <td>34688000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>113.28</td>\n",
       "      <td>114.98</td>\n",
       "      <td>112.86</td>\n",
       "      <td>113.28</td>\n",
       "      <td>37717000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>110.73</td>\n",
       "      <td>112.01</td>\n",
       "      <td>110.73</td>\n",
       "      <td>111.58</td>\n",
       "      <td>39253000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>116.67</td>\n",
       "      <td>117.10</td>\n",
       "      <td>114.98</td>\n",
       "      <td>116.25</td>\n",
       "      <td>79464000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>114.55</td>\n",
       "      <td>117.10</td>\n",
       "      <td>114.13</td>\n",
       "      <td>116.25</td>\n",
       "      <td>36757000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>116.67</td>\n",
       "      <td>117.10</td>\n",
       "      <td>115.40</td>\n",
       "      <td>117.10</td>\n",
       "      <td>23737000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>116.67</td>\n",
       "      <td>116.67</td>\n",
       "      <td>113.28</td>\n",
       "      <td>114.13</td>\n",
       "      <td>43839000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>113.70</td>\n",
       "      <td>115.83</td>\n",
       "      <td>113.70</td>\n",
       "      <td>114.55</td>\n",
       "      <td>31933000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>115.83</td>\n",
       "      <td>117.52</td>\n",
       "      <td>115.40</td>\n",
       "      <td>117.52</td>\n",
       "      <td>33889000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>118.80</td>\n",
       "      <td>118.80</td>\n",
       "      <td>117.95</td>\n",
       "      <td>118.37</td>\n",
       "      <td>27184000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>116.67</td>\n",
       "      <td>117.95</td>\n",
       "      <td>116.25</td>\n",
       "      <td>116.67</td>\n",
       "      <td>24049000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>117.95</td>\n",
       "      <td>118.37</td>\n",
       "      <td>117.52</td>\n",
       "      <td>118.37</td>\n",
       "      <td>24029000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>117.10</td>\n",
       "      <td>119.22</td>\n",
       "      <td>117.10</td>\n",
       "      <td>118.80</td>\n",
       "      <td>37867000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>117.52</td>\n",
       "      <td>121.34</td>\n",
       "      <td>117.10</td>\n",
       "      <td>121.34</td>\n",
       "      <td>55616000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>122.19</td>\n",
       "      <td>122.19</td>\n",
       "      <td>120.07</td>\n",
       "      <td>122.19</td>\n",
       "      <td>40943000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-02-02</td>\n",
       "      <td>122.19</td>\n",
       "      <td>122.61</td>\n",
       "      <td>120.07</td>\n",
       "      <td>120.92</td>\n",
       "      <td>25380000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>123.04</td>\n",
       "      <td>123.89</td>\n",
       "      <td>122.61</td>\n",
       "      <td>123.89</td>\n",
       "      <td>70465000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>120.49</td>\n",
       "      <td>124.31</td>\n",
       "      <td>120.49</td>\n",
       "      <td>123.04</td>\n",
       "      <td>72875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>123.89</td>\n",
       "      <td>125.58</td>\n",
       "      <td>123.46</td>\n",
       "      <td>125.16</td>\n",
       "      <td>55006000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>126.43</td>\n",
       "      <td>126.43</td>\n",
       "      <td>124.74</td>\n",
       "      <td>125.58</td>\n",
       "      <td>38453000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>127.28</td>\n",
       "      <td>128.13</td>\n",
       "      <td>126.86</td>\n",
       "      <td>128.13</td>\n",
       "      <td>61657000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-02-19</td>\n",
       "      <td>126.43</td>\n",
       "      <td>128.13</td>\n",
       "      <td>126.43</td>\n",
       "      <td>128.13</td>\n",
       "      <td>35668000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-02-22</td>\n",
       "      <td>126.86</td>\n",
       "      <td>129.40</td>\n",
       "      <td>126.86</td>\n",
       "      <td>128.13</td>\n",
       "      <td>27003000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-02-23</td>\n",
       "      <td>127.70</td>\n",
       "      <td>128.13</td>\n",
       "      <td>126.43</td>\n",
       "      <td>126.86</td>\n",
       "      <td>22302000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>296.13</td>\n",
       "      <td>296.13</td>\n",
       "      <td>284.24</td>\n",
       "      <td>291.17</td>\n",
       "      <td>113527351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>272.36</td>\n",
       "      <td>291.17</td>\n",
       "      <td>269.88</td>\n",
       "      <td>287.21</td>\n",
       "      <td>149995148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>282.26</td>\n",
       "      <td>288.20</td>\n",
       "      <td>272.85</td>\n",
       "      <td>273.84</td>\n",
       "      <td>103873228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>262.45</td>\n",
       "      <td>273.84</td>\n",
       "      <td>262.45</td>\n",
       "      <td>265.42</td>\n",
       "      <td>121323651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>266.91</td>\n",
       "      <td>269.88</td>\n",
       "      <td>257.50</td>\n",
       "      <td>257.50</td>\n",
       "      <td>117437643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>252.00</td>\n",
       "      <td>253.00</td>\n",
       "      <td>235.50</td>\n",
       "      <td>248.00</td>\n",
       "      <td>160811697</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>258.50</td>\n",
       "      <td>270.00</td>\n",
       "      <td>256.00</td>\n",
       "      <td>270.00</td>\n",
       "      <td>157664726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>257.00</td>\n",
       "      <td>262.50</td>\n",
       "      <td>252.00</td>\n",
       "      <td>255.00</td>\n",
       "      <td>76466709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>268.00</td>\n",
       "      <td>274.00</td>\n",
       "      <td>266.00</td>\n",
       "      <td>267.50</td>\n",
       "      <td>81952933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>276.50</td>\n",
       "      <td>280.00</td>\n",
       "      <td>274.00</td>\n",
       "      <td>277.00</td>\n",
       "      <td>79447199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>279.50</td>\n",
       "      <td>280.00</td>\n",
       "      <td>275.50</td>\n",
       "      <td>280.00</td>\n",
       "      <td>52845633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>284.00</td>\n",
       "      <td>286.00</td>\n",
       "      <td>273.00</td>\n",
       "      <td>273.00</td>\n",
       "      <td>68752306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>263.50</td>\n",
       "      <td>269.00</td>\n",
       "      <td>262.50</td>\n",
       "      <td>267.50</td>\n",
       "      <td>49159956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>273.00</td>\n",
       "      <td>274.00</td>\n",
       "      <td>269.50</td>\n",
       "      <td>274.00</td>\n",
       "      <td>48837923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>276.50</td>\n",
       "      <td>276.50</td>\n",
       "      <td>271.50</td>\n",
       "      <td>271.50</td>\n",
       "      <td>44515034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>273.00</td>\n",
       "      <td>275.50</td>\n",
       "      <td>270.00</td>\n",
       "      <td>275.50</td>\n",
       "      <td>56392754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>283.50</td>\n",
       "      <td>284.00</td>\n",
       "      <td>280.50</td>\n",
       "      <td>283.00</td>\n",
       "      <td>48787346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>285.00</td>\n",
       "      <td>285.50</td>\n",
       "      <td>283.00</td>\n",
       "      <td>285.00</td>\n",
       "      <td>37912826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>287.50</td>\n",
       "      <td>288.00</td>\n",
       "      <td>282.50</td>\n",
       "      <td>283.00</td>\n",
       "      <td>27620430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>280.00</td>\n",
       "      <td>282.00</td>\n",
       "      <td>279.00</td>\n",
       "      <td>279.50</td>\n",
       "      <td>27252858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>278.50</td>\n",
       "      <td>281.50</td>\n",
       "      <td>278.50</td>\n",
       "      <td>278.50</td>\n",
       "      <td>22196596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>281.00</td>\n",
       "      <td>285.50</td>\n",
       "      <td>280.00</td>\n",
       "      <td>285.00</td>\n",
       "      <td>48809688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>287.00</td>\n",
       "      <td>288.50</td>\n",
       "      <td>286.00</td>\n",
       "      <td>287.50</td>\n",
       "      <td>55216434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>283.00</td>\n",
       "      <td>288.50</td>\n",
       "      <td>283.00</td>\n",
       "      <td>286.50</td>\n",
       "      <td>47815280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>303.50</td>\n",
       "      <td>308.50</td>\n",
       "      <td>300.50</td>\n",
       "      <td>306.50</td>\n",
       "      <td>116368201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>308.00</td>\n",
       "      <td>309.00</td>\n",
       "      <td>302.50</td>\n",
       "      <td>304.00</td>\n",
       "      <td>41381669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>300.50</td>\n",
       "      <td>301.50</td>\n",
       "      <td>293.50</td>\n",
       "      <td>295.00</td>\n",
       "      <td>61479357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>294.50</td>\n",
       "      <td>295.00</td>\n",
       "      <td>290.50</td>\n",
       "      <td>294.00</td>\n",
       "      <td>42192580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>300.00</td>\n",
       "      <td>300.00</td>\n",
       "      <td>294.00</td>\n",
       "      <td>295.50</td>\n",
       "      <td>39474478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>295.50</td>\n",
       "      <td>297.00</td>\n",
       "      <td>294.00</td>\n",
       "      <td>294.00</td>\n",
       "      <td>26008090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1045 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close     Volume  Dividends  \\\n",
       "0     2016-01-04  120.92  121.77  117.95  118.37   39169000        0.0   \n",
       "1     2016-01-05  117.95  118.80  116.25  117.10   46381000        0.0   \n",
       "2     2016-01-06  117.10  117.10  114.55  114.98   53617000        0.0   \n",
       "3     2016-01-07  114.13  114.55  110.73  112.86   62539000        0.0   \n",
       "4     2016-01-08  112.01  114.55  112.01  113.70   50123000        0.0   \n",
       "5     2016-01-11  112.86  114.13  110.73  112.86   48345000        0.0   \n",
       "6     2016-01-12  112.86  113.70  111.58  112.86   34688000        0.0   \n",
       "7     2016-01-13  113.28  114.98  112.86  113.28   37717000        0.0   \n",
       "8     2016-01-14  110.73  112.01  110.73  111.58   39253000        0.0   \n",
       "9     2016-01-15  116.67  117.10  114.98  116.25   79464000        0.0   \n",
       "10    2016-01-18  114.55  117.10  114.13  116.25   36757000        0.0   \n",
       "11    2016-01-19  116.67  117.10  115.40  117.10   23737000        0.0   \n",
       "12    2016-01-20  116.67  116.67  113.28  114.13   43839000        0.0   \n",
       "13    2016-01-21  113.70  115.83  113.70  114.55   31933000        0.0   \n",
       "14    2016-01-22  115.83  117.52  115.40  117.52   33889000        0.0   \n",
       "15    2016-01-25  118.80  118.80  117.95  118.37   27184000        0.0   \n",
       "16    2016-01-26  116.67  117.95  116.25  116.67   24049000        0.0   \n",
       "17    2016-01-27  117.95  118.37  117.52  118.37   24029000        0.0   \n",
       "18    2016-01-28  117.10  119.22  117.10  118.80   37867000        0.0   \n",
       "19    2016-01-29  117.52  121.34  117.10  121.34   55616000        0.0   \n",
       "20    2016-02-01  122.19  122.19  120.07  122.19   40943000        0.0   \n",
       "21    2016-02-02  122.19  122.61  120.07  120.92   25380000        0.0   \n",
       "22    2016-02-03  123.04  123.89  122.61  123.89   70465000        0.0   \n",
       "23    2016-02-15  120.49  124.31  120.49  123.04   72875000        0.0   \n",
       "24    2016-02-16  123.89  125.58  123.46  125.16   55006000        0.0   \n",
       "25    2016-02-17  126.43  126.43  124.74  125.58   38453000        0.0   \n",
       "26    2016-02-18  127.28  128.13  126.86  128.13   61657000        0.0   \n",
       "27    2016-02-19  126.43  128.13  126.43  128.13   35668000        0.0   \n",
       "28    2016-02-22  126.86  129.40  126.86  128.13   27003000        0.0   \n",
       "29    2016-02-23  127.70  128.13  126.43  126.86   22302000        0.0   \n",
       "...          ...     ...     ...     ...     ...        ...        ...   \n",
       "1015  2020-03-12  296.13  296.13  284.24  291.17  113527351        0.0   \n",
       "1016  2020-03-13  272.36  291.17  269.88  287.21  149995148        0.0   \n",
       "1017  2020-03-16  282.26  288.20  272.85  273.84  103873228        0.0   \n",
       "1018  2020-03-17  262.45  273.84  262.45  265.42  121323651        0.0   \n",
       "1019  2020-03-18  266.91  269.88  257.50  257.50  117437643        0.0   \n",
       "1020  2020-03-19  252.00  253.00  235.50  248.00  160811697        2.5   \n",
       "1021  2020-03-20  258.50  270.00  256.00  270.00  157664726        0.0   \n",
       "1022  2020-03-23  257.00  262.50  252.00  255.00   76466709        0.0   \n",
       "1023  2020-03-24  268.00  274.00  266.00  267.50   81952933        0.0   \n",
       "1024  2020-03-25  276.50  280.00  274.00  277.00   79447199        0.0   \n",
       "1025  2020-03-26  279.50  280.00  275.50  280.00   52845633        0.0   \n",
       "1026  2020-03-27  284.00  286.00  273.00  273.00   68752306        0.0   \n",
       "1027  2020-03-30  263.50  269.00  262.50  267.50   49159956        0.0   \n",
       "1028  2020-03-31  273.00  274.00  269.50  274.00   48837923        0.0   \n",
       "1029  2020-04-01  276.50  276.50  271.50  271.50   44515034        0.0   \n",
       "1030  2020-04-06  273.00  275.50  270.00  275.50   56392754        0.0   \n",
       "1031  2020-04-07  283.50  284.00  280.50  283.00   48787346        0.0   \n",
       "1032  2020-04-08  285.00  285.50  283.00  285.00   37912826        0.0   \n",
       "1033  2020-04-09  287.50  288.00  282.50  283.00   27620430        0.0   \n",
       "1034  2020-04-10  280.00  282.00  279.00  279.50   27252858        0.0   \n",
       "1035  2020-04-13  278.50  281.50  278.50  278.50   22196596        0.0   \n",
       "1036  2020-04-14  281.00  285.50  280.00  285.00   48809688        0.0   \n",
       "1037  2020-04-15  287.00  288.50  286.00  287.50   55216434        0.0   \n",
       "1038  2020-04-16  283.00  288.50  283.00  286.50   47815280        0.0   \n",
       "1039  2020-04-17  303.50  308.50  300.50  306.50  116368201        0.0   \n",
       "1040  2020-04-20  308.00  309.00  302.50  304.00   41381669        0.0   \n",
       "1041  2020-04-21  300.50  301.50  293.50  295.00   61479357        0.0   \n",
       "1042  2020-04-22  294.50  295.00  290.50  294.00   42192580        0.0   \n",
       "1043  2020-04-23  300.00  300.00  294.00  295.50   39474478        0.0   \n",
       "1044  2020-04-24  295.50  297.00  294.00  294.00   26008090        0.0   \n",
       "\n",
       "      Stock Splits  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "5                0  \n",
       "6                0  \n",
       "7                0  \n",
       "8                0  \n",
       "9                0  \n",
       "10               0  \n",
       "11               0  \n",
       "12               0  \n",
       "13               0  \n",
       "14               0  \n",
       "15               0  \n",
       "16               0  \n",
       "17               0  \n",
       "18               0  \n",
       "19               0  \n",
       "20               0  \n",
       "21               0  \n",
       "22               0  \n",
       "23               0  \n",
       "24               0  \n",
       "25               0  \n",
       "26               0  \n",
       "27               0  \n",
       "28               0  \n",
       "29               0  \n",
       "...            ...  \n",
       "1015             0  \n",
       "1016             0  \n",
       "1017             0  \n",
       "1018             0  \n",
       "1019             0  \n",
       "1020             0  \n",
       "1021             0  \n",
       "1022             0  \n",
       "1023             0  \n",
       "1024             0  \n",
       "1025             0  \n",
       "1026             0  \n",
       "1027             0  \n",
       "1028             0  \n",
       "1029             0  \n",
       "1030             0  \n",
       "1031             0  \n",
       "1032             0  \n",
       "1033             0  \n",
       "1034             0  \n",
       "1035             0  \n",
       "1036             0  \n",
       "1037             0  \n",
       "1038             0  \n",
       "1039             0  \n",
       "1040             0  \n",
       "1041             0  \n",
       "1042             0  \n",
       "1043             0  \n",
       "1044             0  \n",
       "\n",
       "[1045 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120.92],\n",
       "       [117.95],\n",
       "       [117.1 ],\n",
       "       ...,\n",
       "       [294.5 ],\n",
       "       [300.  ],\n",
       "       [295.5 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04402679],\n",
       "       [0.03119464],\n",
       "       [0.02752214],\n",
       "       ...,\n",
       "       [0.79399438],\n",
       "       [0.81775762],\n",
       "       [0.79831497]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []   #預測點的前 60 天的資料\n",
    "y_train = []   #預測點\n",
    "for i in range(60, len(training_set)):  # 1258 是訓練集總數\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)  # 轉成numpy array的格式，以利輸入 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████████████████▎  | 293.1 MB 196 kB/s eta 0:02:19   |▏                               | 1.4 MB 237 kB/s eta 0:22:26     |▎                               | 3.1 MB 203 kB/s eta 0:25:56     |▎                               | 3.2 MB 173 kB/s eta 0:30:30     |▌                               | 5.3 MB 139 kB/s eta 0:37:31     |▋                               | 6.1 MB 152 kB/s eta 0:34:22     |▊                               | 6.8 MB 124 kB/s eta 0:41:51     |▉                               | 8.1 MB 152 kB/s eta 0:34:05     |▉                               | 8.5 MB 140 kB/s eta 0:37:06     |█                               | 8.9 MB 129 kB/s eta 0:40:02     |█▏                              | 11.4 MB 142 kB/s eta 0:36:15     |█▏                              | 12.1 MB 63 kB/s eta 1:20:24     |█▎                              | 12.3 MB 93 kB/s eta 0:54:44     |█▎                              | 13.3 MB 126 kB/s eta 0:40:30     |█▌                              | 14.5 MB 150 kB/s eta 0:33:54     |█▋                              | 15.6 MB 172 kB/s eta 0:29:23     |█▋                              | 15.9 MB 222 kB/s eta 0:22:51     |█▊                              | 17.4 MB 237 kB/s eta 0:21:17     |█▊                              | 17.5 MB 190 kB/s eta 0:26:30     |█▊                              | 17.7 MB 242 kB/s eta 0:20:50     |█▉                              | 18.1 MB 189 kB/s eta 0:26:37     |██                              | 19.7 MB 93 kB/s eta 0:53:21     |██                              | 21.1 MB 198 kB/s eta 0:25:10     |██▎                             | 22.3 MB 250 kB/s eta 0:19:52     |██▎                             | 22.5 MB 176 kB/s eta 0:28:09     |██▌                             | 25.2 MB 170 kB/s eta 0:28:51     |██▌                             | 25.4 MB 162 kB/s eta 0:30:13     |██▌                             | 25.6 MB 185 kB/s eta 0:26:30     |███                             | 30.8 MB 106 kB/s eta 0:45:14     |███▏                            | 31.9 MB 157 kB/s eta 0:30:32     |███▏                            | 32.0 MB 157 kB/s eta 0:30:31     |███▎                            | 32.4 MB 213 kB/s eta 0:22:31     |███▍                            | 34.0 MB 237 kB/s eta 0:20:04     |███▍                            | 34.2 MB 330 kB/s eta 0:14:26     |███▍                            | 34.4 MB 330 kB/s eta 0:14:25     |███▋                            | 36.1 MB 173 kB/s eta 0:27:22     |███▊                            | 36.9 MB 170 kB/s eta 0:27:42     |████                            | 39.4 MB 218 kB/s eta 0:21:27     |████                            | 39.9 MB 223 kB/s eta 0:20:58     |████                            | 40.0 MB 148 kB/s eta 0:31:28     |████                            | 40.1 MB 148 kB/s eta 0:31:28     |████                            | 40.8 MB 227 kB/s eta 0:20:29     |████▎                           | 42.3 MB 226 kB/s eta 0:20:27     |████▍                           | 44.0 MB 203 kB/s eta 0:22:36     |████▍                           | 44.1 MB 159 kB/s eta 0:28:48     |████▌                           | 45.2 MB 117 kB/s eta 0:39:00     |████▌                           | 45.2 MB 164 kB/s eta 0:27:49     |████▉                           | 48.5 MB 262 kB/s eta 0:17:14     |████▉                           | 48.9 MB 242 kB/s eta 0:18:40     |█████                           | 49.7 MB 131 kB/s eta 0:34:23     |█████                           | 51.1 MB 155 kB/s eta 0:28:55     |█████▎                          | 52.3 MB 254 kB/s eta 0:17:34     |█████▍                          | 53.6 MB 353 kB/s eta 0:12:34     |█████▍                          | 54.3 MB 353 kB/s eta 0:12:33     |█████▍                          | 54.5 MB 329 kB/s eta 0:13:27     |█████▌                          | 54.8 MB 389 kB/s eta 0:11:22     |█████▌                          | 55.3 MB 283 kB/s eta 0:15:34     |█████▉                          | 58.6 MB 255 kB/s eta 0:17:05     |██████                          | 59.7 MB 222 kB/s eta 0:19:32     |██████▏                         | 61.2 MB 283 kB/s eta 0:15:14     |██████▏                         | 61.9 MB 180 kB/s eta 0:23:56     |██████▍                         | 64.2 MB 146 kB/s eta 0:29:14     |██████▌                         | 64.6 MB 262 kB/s eta 0:16:14     |██████▌                         | 64.8 MB 292 kB/s eta 0:14:33     |██████▋                         | 66.0 MB 108 kB/s eta 0:38:55     |██████▋                         | 66.5 MB 155 kB/s eta 0:27:17     |██████▋                         | 66.7 MB 189 kB/s eta 0:22:21     |██████▊                         | 67.3 MB 200 kB/s eta 0:21:00     |██████▉                         | 68.5 MB 179 kB/s eta 0:23:25     |███████                         | 69.1 MB 179 kB/s eta 0:23:21     |███████                         | 70.3 MB 138 kB/s eta 0:30:00     |███████▏                        | 72.2 MB 163 kB/s eta 0:25:21     |███████▎                        | 72.9 MB 123 kB/s eta 0:33:28     |███████▍                        | 74.3 MB 145 kB/s eta 0:28:10     |███████▊                        | 77.3 MB 204 kB/s eta 0:19:48     |███████▊                        | 77.7 MB 212 kB/s eta 0:19:00     |████████                        | 80.1 MB 182 kB/s eta 0:21:55     |████████▉                       | 88.2 MB 170 kB/s eta 0:22:43     |████████▉                       | 88.2 MB 170 kB/s eta 0:22:42     |████████▉                       | 88.9 MB 201 kB/s eta 0:19:07     |█████████                       | 89.0 MB 201 kB/s eta 0:19:07     |█████████                       | 90.7 MB 176 kB/s eta 0:21:39     |█████████▏                      | 91.8 MB 272 kB/s eta 0:14:00     |█████████▏                      | 91.9 MB 272 kB/s eta 0:13:59     |█████████▏                      | 92.3 MB 300 kB/s eta 0:12:39     |█████████▊                      | 97.6 MB 89 kB/s eta 0:41:22     |█████████▉                      | 98.6 MB 172 kB/s eta 0:21:26     |██████████▎                     | 102.6 MB 182 kB/s eta 0:19:53     |██████████▎                     | 103.3 MB 165 kB/s eta 0:21:54     |██████████▍                     | 103.8 MB 110 kB/s eta 0:32:49     |██████████▍                     | 104.2 MB 95 kB/s eta 0:37:41     |██████████▌                     | 104.8 MB 126 kB/s eta 0:28:27     |███████████▌                    | 115.0 MB 132 kB/s eta 0:25:47     |█████████████                   | 129.9 MB 148 kB/s eta 0:21:25     |█████████████▍                  | 133.8 MB 126 kB/s eta 0:24:38     |██████████████▍                 | 144.3 MB 144 kB/s eta 0:20:23     |███████████████▎                | 153.2 MB 164 kB/s eta 0:16:56     |█████████████████               | 170.6 MB 137 kB/s eta 0:18:05     |█████████████████               | 171.2 MB 102 kB/s eta 0:24:18     |█████████████████▏              | 172.2 MB 123 kB/s eta 0:20:02     |█████████████████▏              | 172.2 MB 123 kB/s eta 0:20:02     |█████████████████▌              | 175.7 MB 115 kB/s eta 0:20:49     |█████████████████▋              | 175.8 MB 132 kB/s eta 0:18:08     |█████████████████▉              | 178.0 MB 76 kB/s eta 0:31:03     |███████████████████▍            | 194.7 MB 157 kB/s eta 0:13:21     |███████████████████▋            | 196.6 MB 149 kB/s eta 0:13:50     |████████████████████▋           | 206.1 MB 146 kB/s eta 0:13:03     |█████████████████████▌          | 214.8 MB 167 kB/s eta 0:10:30     |██████████████████████▌         | 225.1 MB 152 kB/s eta 0:10:25     |███████████████████████▍        | 234.0 MB 219 kB/s eta 0:06:35     |███████████████████████▉        | 238.7 MB 150 kB/s eta 0:09:03     |███████████████████████▉        | 239.1 MB 170 kB/s eta 0:07:57     |████████████████████████▏       | 241.9 MB 87 kB/s eta 0:14:58     |████████████████████████▉       | 248.1 MB 146 kB/s eta 0:08:15     |█████████████████████████       | 249.6 MB 176 kB/s eta 0:06:42     |█████████████████████████▍      | 254.5 MB 301 kB/s eta 0:03:38     |█████████████████████████▋      | 256.0 MB 113 kB/s eta 0:09:27     |██████████████████████████      | 261.3 MB 120 kB/s eta 0:08:12     |████████████████████████████▌   | 285.0 MB 165 kB/s eta 0:03:34     |█████████████████████████████▏  | 291.7 MB 225 kB/s eta 0:02:08     |█████████████████████████████▎  | 293.1 MB 196 kB/s eta 0:02:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 320.4 MB 35 kB/s  eta 0:00:01     |█████████████████████████████▋  | 296.2 MB 134 kB/s eta 0:03:00     |███████████████████████████████ | 311.1 MB 134 kB/s eta 0:01:10     |███████████████████████████████▎| 312.8 MB 212 kB/s eta 0:00:36     |████████████████████████████████| 319.5 MB 227 kB/s eta 0:00:04\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 97 kB/s eta 0:00:010\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 121 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 74 kB/s eta 0:00:013\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 91 kB/s eta 0:00:011     |█████████████████▏              | 3.6 MB 101 kB/s eta 0:00:32\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 140 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.12.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 84 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 126 kB/s eta 0:00:01    |████████████                    | 7.5 MB 139 kB/s eta 0:01:31     |██████████████▊                 | 9.3 MB 146 kB/s eta 0:01:15     |████████████████████████████▌   | 17.9 MB 131 kB/s eta 0:00:17\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 123 kB/s eta 0:00:01    |██▏                             | 1.8 MB 200 kB/s eta 0:02:02     |█████████████████████████▋      | 20.9 MB 139 kB/s eta 0:00:37\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 118 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 128 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.12.4-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 102 kB/s eta 0:00:01     |████████████████████████████▊   | 1.1 MB 102 kB/s eta 0:00:02\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 173 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.20.0-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 132 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=41.0.0\n",
      "  Downloading setuptools-49.2.1-py3-none-any.whl (789 kB)\n",
      "\u001b[K     |████████████████████████████████| 789 kB 108 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 59 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 117 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.14.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.1)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 76 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2018.8.13)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.23)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 47 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.4)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4001 sha256=3306efab084c4921bdbbdf1fa598f69f3db3b7d2ccd87b100441ec1e54eac1b4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=18689 sha256=186ec3cf5755d7af5f8eee9ce1325490c43a639ec9079936b427beeafe85e278\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=117798 sha256=9c0d9f2bc52700cae19f3d45a6a7c23efa7e80b79ff59021d64f012366e0cbd6\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "Successfully built termcolor wrapt absl-py\n",
      "\u001b[31mERROR: twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: twisted 18.7.0 has requirement Automat>=0.3.0, but you'll have automat 0.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: termcolor, numpy, six, h5py, opt-einsum, tensorflow-estimator, tensorboard-plugin-wit, requests, oauthlib, requests-oauthlib, setuptools, cachetools, rsa, google-auth, google-auth-oauthlib, grpcio, protobuf, zipp, importlib-metadata, markdown, absl-py, tensorboard, keras-preprocessing, google-pasta, wrapt, scipy, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.19.1\n",
      "    Uninstalling requests-2.19.1:\n",
      "      Successfully uninstalled requests-2.19.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.0.0\n",
      "    Uninstalling setuptools-40.0.0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled setuptools-40.0.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.2.0\n",
      "    Uninstalling scipy-1.2.0:\n",
      "      Successfully uninstalled scipy-1.2.0\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.20.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 importlib-metadata-1.7.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.12.4 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 setuptools-49.2.1 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 wrapt-1.12.1 zipp-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras) (1.4.1)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 491 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from Keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->Keras) (1.15.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44632 sha256=34ad0615f45f8209eff5dcc6503fed8c601210ab7e085741664d0b5e881864f7\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, Keras\n",
      "Successfully installed Keras-2.4.3 pyyaml-5.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ufunc has the wrong size, try recompiling. Expected 192, got 216",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2fefbcffa425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the Keras libraries and packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mINFINITE\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mINFINITE_CARDINALITY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDispatchServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkerServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections_abc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfast_tensor_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0m_FAST_TENSOR_UTIL_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m__init__.pxd\u001b[0m in \u001b[0;36minit tensorflow.python.framework.fast_tensor_util\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ufunc has the wrong size, try recompiling. Expected 192, got 216"
     ]
    }
   ],
   "source": [
    "# Import the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
